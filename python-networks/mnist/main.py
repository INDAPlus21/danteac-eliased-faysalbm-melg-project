# each neuron holds a number, and each connection a weight 
# in the mnist 3b1b example, there are 784 input neurons (one for each 
# pixel in the image)
# feedforward is just multiply all the previous activations and weights 
# all the weights are ALL the weights in the connection, ordered like 
# [w00, w01... wjk]

# each neuron has an activation a, and each connection a weight w 
# we can only change the weights and biases, activations are direct calculation 
# of the weights and biases  
# what!?!? "The squished 'd' is the partial derivative sign" 
# make a really simple neural network with the addition and multiplcation derivatives z/ follow the entirety of Lex Fridman's course 
# gates are important because operations can be broken down into addition, multiplication, and max (you basically only need these)
# forward propagation gets us a value for each gate (woah!
# what you're modifying is the weights